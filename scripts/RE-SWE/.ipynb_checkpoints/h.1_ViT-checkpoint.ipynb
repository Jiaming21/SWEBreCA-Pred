{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7576b9bb-0569-4c6e-9bb3-f4622061dbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, matthews_corrcoef, recall_score, precision_score\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "from torch.utils.data import TensorDataset, random_split, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import torchvision.models as models\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch import nn, einsum\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31ac8564-b6a5-4000-9fbb-9c8a659861cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_882/2980736523.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  loaded_datasets_info = torch.load('/root/autodl-tmp/imgs/RE-SWE/saved_datasets_RE-SWE.pth')\n"
     ]
    }
   ],
   "source": [
    "loaded_datasets_info = torch.load('/root/autodl-tmp/imgs/RE-SWE/saved_datasets_RE-SWE.pth')\n",
    "train_dataset = loaded_datasets_info['train_dataset']\n",
    "val_dataset = loaded_datasets_info['val_dataset']\n",
    "test_dataset = loaded_datasets_info['test_dataset']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50a3ee7f-f979-4d4f-bb01-f23be2c0902f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "loaded_train_dataset = DataLoader(train_dataset, batch_size = batch_size, shuffle = False)\n",
    "loaded_val_dataset = DataLoader(val_dataset, batch_size = batch_size, shuffle = False)\n",
    "loaded_test_dataset = DataLoader(test_dataset, batch_size = batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f6798be-983d-47b7-a66a-c8bc325ccac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# auxillary function: generate array\n",
    "def pair(t):\n",
    "    return t if isinstance(t, tuple) else (t, t)\n",
    "\n",
    "# regularize layer class encapsulation\n",
    "# (1) regularized layer, FFN, Attention\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "    \n",
    "# FFN\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "# Attention\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head *  heads\n",
    "        project_out = not (heads == 1 and dim_head == dim)\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.attend = nn.Softmax(dim = -1)\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, n, _, h = *x.shape, self.heads\n",
    "        qkv = self.to_qkv(x).chunk(3, dim = -1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), qkv)\n",
    "        dots = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n",
    "        attn = self.attend(dots)\n",
    "        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        return self.to_out(out)\n",
    "\n",
    "# (2) build transformer\n",
    "# based on PreNorm, Attention and FFN to build transformer\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                PreNorm(dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout)),\n",
    "                PreNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout))\n",
    "            ]))\n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "        return x\n",
    "\n",
    "# (3) build ViT\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, pool = 'cls', channels = 3, dim_head = 64, dropout = 0., emb_dropout = 0.):\n",
    "        super().__init__()\n",
    "        image_height, image_width = pair(image_size)\n",
    "        patch_height, patch_width = pair(patch_size)\n",
    "\n",
    "        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n",
    "        # patch amount \n",
    "        num_patches = (image_height // patch_height) * (image_width // patch_width)\n",
    "        # patch dimension\n",
    "        patch_dim = channels * patch_height * patch_width\n",
    "        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n",
    "        # define patch embedding\n",
    "        self.to_patch_embedding = nn.Sequential(\n",
    "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_height, p2 = patch_width),\n",
    "            nn.Linear(patch_dim, dim),\n",
    "        )\n",
    "        # define positional embedding\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
    "        # define class vector\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "        self.dropout = nn.Dropout(emb_dropout)\n",
    "\n",
    "        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n",
    "\n",
    "        self.pool = pool\n",
    "        self.to_latent = nn.Identity()\n",
    "        # define MLP\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, num_classes)\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    # ViT forward \n",
    "    def forward(self, img):\n",
    "        # patch embedding \n",
    "        x = self.to_patch_embedding(img)\n",
    "        b, n, _ = x.shape\n",
    "        # add class vector\n",
    "        cls_tokens = repeat(self.cls_token, '() n d -> b n d', b = b)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        # add positional encoding\n",
    "        x += self.pos_embedding[:, :(n + 1)]\n",
    "        # dropout\n",
    "        x = self.dropout(x)\n",
    "        # input into transformer\n",
    "        x = self.transformer(x)\n",
    "        x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0]\n",
    "        x = self.to_latent(x)\n",
    "        # MLP\n",
    "        return self.sigmoid(self.mlp_head(x))\n",
    "\n",
    "model = ViT(\n",
    "            image_size = 224,\n",
    "            patch_size = 32,\n",
    "            num_classes = 1,\n",
    "            dim = 1024,\n",
    "            depth = 6,\n",
    "            heads = 16,\n",
    "            mlp_dim = 2048,\n",
    "            dropout = 0.1,\n",
    "            emb_dropout = 0.1\n",
    "            )\n",
    "\n",
    "device = \"cuda\"\n",
    "model = model.to(device)\n",
    "criterion = torch.nn.BCELoss()\n",
    "num_epochs = 10\n",
    "\n",
    "# Scheduler https://arxiv.org/pdf/1812.01187.pdf\n",
    "lr = 0.001\n",
    "lf = lambda x: ((1 + math.cos(x * math.pi / num_epochs)) / 2) * (1 - lr) + lr # cosine\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65456f70-92e5-47a9-ae62-d9ade276afe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 6.368084859389525\n",
      "Epoch 2/10, Loss: 4.664853060474762\n",
      "Epoch 3/10, Loss: 3.922475673831426\n",
      "Epoch 4/10, Loss: 6.696890798898844\n",
      "Epoch 5/10, Loss: 4.16070296214177\n",
      "Epoch 6/10, Loss: 3.3805360501775374\n",
      "Epoch 7/10, Loss: 4.399468357173296\n",
      "Epoch 8/10, Loss: 3.451663742844875\n",
      "Epoch 9/10, Loss: 3.228579100507956\n",
      "Epoch 10/10, Loss: 3.409356368848911\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch_indx, (inputs, labels) in enumerate(loaded_train_dataset):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    # Print average loss for the epoch\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss / (len(loaded_train_dataset) / batch_size)}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99d7da84-7952-4d88-b8df-8a4b4c291896",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_probabilities = []\n",
    "true_labels = []\n",
    "with torch.set_grad_enabled(False):\n",
    "    for batch_indx, (inputs, labels) in enumerate(loaded_val_dataset):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)      \n",
    "        outputs = model(inputs)\n",
    "        predicted_probabilities.extend(outputs.tolist())\n",
    "        true_labels.extend(labels.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d526eea0-ce25-4f55-b459-1d9598fa7101",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics_output(preds,labels):\n",
    "    true_labels = np.array(labels)\n",
    "    predicted_probs = np.array(preds)\n",
    "    binary_predictions = (predicted_probs >= 0.5).astype(int)\n",
    "    auc = roc_auc_score(true_labels, predicted_probs)\n",
    "    conf_matrix = confusion_matrix(true_labels, binary_predictions)\n",
    "    tn, fp, fn, tp = conf_matrix.ravel()\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    specificity = tn / (tn + fp)\n",
    "    accuracy = accuracy_score(true_labels, binary_predictions)\n",
    "    f1 = f1_score(true_labels, binary_predictions)\n",
    "    mcc = matthews_corrcoef(true_labels, binary_predictions)  \n",
    "    return (auc, sensitivity, specificity, accuracy, f1, mcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "117dd4b5-4bc2-4e2d-ba8b-8461b3768eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9697916666666667 0.9166666666666666 0.8 0.84375 0.8148148148148148 0.6952687917708212\n"
     ]
    }
   ],
   "source": [
    "roc_auc, metrics_sn, metrics_sp, metrics_ACC, metrics_F1, metrics_MCC = metrics_output(predicted_probabilities, true_labels)\n",
    "print(roc_auc, metrics_sn, metrics_sp, metrics_ACC, metrics_F1, metrics_MCC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10d4e2e9-add9-433b-8d95-fdb0c0d65a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('/root/autodl-tmp/ROC/RE-SWE/ViT/y_val_pred.npy', predicted_probabilities)\n",
    "np.save('/root/autodl-tmp/ROC/RE-SWE/ViT/y_val.npy', true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4d442e3-9ff0-47d4-962b-c36efb20eed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_probabilities = []  \n",
    "true_labels = []  \n",
    "with torch.set_grad_enabled(False): \n",
    "    for batch_indx, (inputs, labels) in enumerate(loaded_test_dataset):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)    \n",
    "        outputs = model(inputs)\n",
    "        predicted_probabilities.extend(outputs.tolist())\n",
    "        true_labels.extend(labels.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54eaaff7-6208-4ebd-89d5-87c0f72b03c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9175191815856778 0.9130434782608695 0.8235294117647058 0.875 0.8936170212765957 0.7432561251138006\n"
     ]
    }
   ],
   "source": [
    "roc_auc, metrics_sn, metrics_sp, metrics_ACC, metrics_F1, metrics_MCC = metrics_output(predicted_probabilities, true_labels)\n",
    "print(roc_auc, metrics_sn, metrics_sp, metrics_ACC, metrics_F1, metrics_MCC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8cc95a2-b6d1-4868-a11e-142e33f214ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('/root/autodl-tmp/ROC/RE-SWE/ViT/y_test_pred.npy', predicted_probabilities)\n",
    "np.save('/root/autodl-tmp/ROC/RE-SWE/ViT/y_test.npy', true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75e4d07-245f-43ba-9ae1-a63e1dd6d76c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a956286e-d8c6-426f-940e-1743a079d7cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f161e144-a1f3-4fce-a408-5ecae0874415",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d723b9-584c-4394-8b1d-bd1c137e6c4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e248f8-257a-437b-9163-553130ced03b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514188bb-94de-431e-ba52-0cf86c82fffd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992e69f6-98ab-432a-ba5d-6d2c0de60f9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558e4b40-12b9-40cb-8239-7deac95ca585",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0c4216-b957-4edb-b4e4-b51f6931c09e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad56716-5263-420d-9fe0-5026b8ba3598",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca800dfe-31a4-4491-a0a5-0391655a995b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597baf6a-5184-45bc-9e36-1ef2aea3e3ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6568bee5-4e56-4fe5-9aa9-ce33f5d271df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe494427-c35e-49a9-aab5-b413af02732b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
