{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6ae13e9-2b65-4e62-9610-38928409afb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, matthews_corrcoef, recall_score, precision_score\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "from torch.utils.data import TensorDataset, random_split, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import torchvision.models as models\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from collections import OrderedDict\n",
    "from functools import partial\n",
    "from typing import Callable, Optional\n",
    "from torch import Tensor\n",
    "\n",
    "import re\n",
    "from typing import Any, List, Tuple\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint as cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7524b75f-c06f-4b59-ae6a-5274de437689",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_867/721222337.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  loaded_datasets_info = torch.load('/root/autodl-tmp/imgs/RE-US/saved_datasets_RE-US.pth')\n"
     ]
    }
   ],
   "source": [
    "loaded_datasets_info = torch.load('/root/autodl-tmp/imgs/RE-US/saved_datasets_RE-US.pth')\n",
    "train_dataset = loaded_datasets_info['train_dataset']\n",
    "val_dataset = loaded_datasets_info['val_dataset']\n",
    "test_dataset = loaded_datasets_info['test_dataset']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d070bec3-d4ab-478f-b5a6-571f39a069b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "loaded_train_dataset = DataLoader(train_dataset, batch_size = batch_size, shuffle = False)\n",
    "loaded_val_dataset = DataLoader(val_dataset, batch_size = batch_size, shuffle = False)\n",
    "loaded_test_dataset = DataLoader(test_dataset, batch_size = batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd77db92-43c1-4688-b947-fa83daa3c763",
   "metadata": {},
   "outputs": [],
   "source": [
    "class _DenseLayer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_c: int,\n",
    "                 growth_rate: int,\n",
    "                 bn_size: int,\n",
    "                 drop_rate: float,\n",
    "                 memory_efficient: bool = False):\n",
    "        super(_DenseLayer, self).__init__()\n",
    "\n",
    "        self.add_module(\"norm1\", nn.BatchNorm2d(input_c))\n",
    "        self.add_module(\"relu1\", nn.ReLU(inplace=True))\n",
    "        self.add_module(\"conv1\", nn.Conv2d(in_channels=input_c,\n",
    "                                           out_channels=bn_size * growth_rate,\n",
    "                                           kernel_size=1,\n",
    "                                           stride=1,\n",
    "                                           bias=False))\n",
    "        self.add_module(\"norm2\", nn.BatchNorm2d(bn_size * growth_rate))\n",
    "        self.add_module(\"relu2\", nn.ReLU(inplace=True))\n",
    "        self.add_module(\"conv2\", nn.Conv2d(bn_size * growth_rate,\n",
    "                                           growth_rate,\n",
    "                                           kernel_size=3,\n",
    "                                           stride=1,\n",
    "                                           padding=1,\n",
    "                                           bias=False))\n",
    "        self.drop_rate = drop_rate\n",
    "        self.memory_efficient = memory_efficient\n",
    "\n",
    "    def bn_function(self, inputs: List[Tensor]) -> Tensor:\n",
    "        concat_features = torch.cat(inputs, 1)\n",
    "        bottleneck_output = self.conv1(self.relu1(self.norm1(concat_features)))\n",
    "        return bottleneck_output\n",
    "\n",
    "    @staticmethod\n",
    "    def any_requires_grad(inputs: List[Tensor]) -> bool:\n",
    "        for tensor in inputs:\n",
    "            if tensor.requires_grad:\n",
    "                return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    @torch.jit.unused\n",
    "    def call_checkpoint_bottleneck(self, inputs: List[Tensor]) -> Tensor:\n",
    "        def closure(*inp):\n",
    "            return self.bn_function(inp)\n",
    "\n",
    "        return cp.checkpoint(closure, *inputs)\n",
    "\n",
    "    def forward(self, inputs: Tensor) -> Tensor:\n",
    "        if isinstance(inputs, Tensor):\n",
    "            prev_features = [inputs]\n",
    "        else:\n",
    "            prev_features = inputs\n",
    "\n",
    "        if self.memory_efficient and self.any_requires_grad(prev_features):\n",
    "            if torch.jit.is_scripting():\n",
    "                raise Exception(\"memory efficient not supported in JIT\")\n",
    "\n",
    "            bottleneck_output = self.call_checkpoint_bottleneck(prev_features)\n",
    "        else:\n",
    "            bottleneck_output = self.bn_function(prev_features)\n",
    "\n",
    "        new_features = self.conv2(self.relu2(self.norm2(bottleneck_output)))\n",
    "        if self.drop_rate > 0:\n",
    "            new_features = F.dropout(new_features,\n",
    "                                     p=self.drop_rate,\n",
    "                                     training=self.training)\n",
    "\n",
    "        return new_features\n",
    "\n",
    "\n",
    "class _DenseBlock(nn.ModuleDict):\n",
    "    _version = 2\n",
    "\n",
    "    def __init__(self,\n",
    "                 num_layers: int,\n",
    "                 input_c: int,\n",
    "                 bn_size: int,\n",
    "                 growth_rate: int,\n",
    "                 drop_rate: float,\n",
    "                 memory_efficient: bool = False):\n",
    "        super(_DenseBlock, self).__init__()\n",
    "        for i in range(num_layers):\n",
    "            layer = _DenseLayer(input_c + i * growth_rate,\n",
    "                                growth_rate=growth_rate,\n",
    "                                bn_size=bn_size,\n",
    "                                drop_rate=drop_rate,\n",
    "                                memory_efficient=memory_efficient)\n",
    "            self.add_module(\"denselayer%d\" % (i + 1), layer)\n",
    "\n",
    "    def forward(self, init_features: Tensor) -> Tensor:\n",
    "        features = [init_features]\n",
    "        for name, layer in self.items():\n",
    "            new_features = layer(features)\n",
    "            features.append(new_features)\n",
    "        return torch.cat(features, 1)\n",
    "\n",
    "\n",
    "class _Transition(nn.Sequential):\n",
    "    def __init__(self,\n",
    "                 input_c: int,\n",
    "                 output_c: int):\n",
    "        super(_Transition, self).__init__()\n",
    "        self.add_module(\"norm\", nn.BatchNorm2d(input_c))\n",
    "        self.add_module(\"relu\", nn.ReLU(inplace=True))\n",
    "        self.add_module(\"conv\", nn.Conv2d(input_c,\n",
    "                                          output_c,\n",
    "                                          kernel_size=1,\n",
    "                                          stride=1,\n",
    "                                          bias=False))\n",
    "        self.add_module(\"pool\", nn.AvgPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "\n",
    "class DenseNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Densenet-BC model class for imagenet\n",
    "\n",
    "    Args:\n",
    "        growth_rate (int) - how many filters to add each layer (`k` in paper)\n",
    "        block_config (list of 4 ints) - how many layers in each pooling block\n",
    "        num_init_features (int) - the number of filters to learn in the first convolution layer\n",
    "        bn_size (int) - multiplicative factor for number of bottle neck layers\n",
    "          (i.e. bn_size * k features in the bottleneck layer)\n",
    "        drop_rate (float) - dropout rate after each dense layer\n",
    "        num_classes (int) - number of classification classes\n",
    "        memory_efficient (bool) - If True, uses checkpointing. Much more memory efficient\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 growth_rate: int = 32,\n",
    "                 block_config: Tuple[int, int, int, int] = (6, 12, 24, 16),\n",
    "                 num_init_features: int = 64,\n",
    "                 bn_size: int = 4,\n",
    "                 drop_rate: float = 0,\n",
    "                 num_classes: int = 1000,\n",
    "                 memory_efficient: bool = False):\n",
    "        super(DenseNet, self).__init__()\n",
    "\n",
    "        # first conv+bn+relu+pool\n",
    "        self.features = nn.Sequential(OrderedDict([\n",
    "            (\"conv0\", nn.Conv2d(3, num_init_features, kernel_size=7, stride=2, padding=3, bias=False)),\n",
    "            (\"norm0\", nn.BatchNorm2d(num_init_features)),\n",
    "            (\"relu0\", nn.ReLU(inplace=True)),\n",
    "            (\"pool0\", nn.MaxPool2d(kernel_size=3, stride=2, padding=1)),\n",
    "        ]))\n",
    "\n",
    "        # each dense block\n",
    "        num_features = num_init_features\n",
    "        for i, num_layers in enumerate(block_config):\n",
    "            block = _DenseBlock(num_layers=num_layers,\n",
    "                                input_c=num_features,\n",
    "                                bn_size=bn_size,\n",
    "                                growth_rate=growth_rate,\n",
    "                                drop_rate=drop_rate,\n",
    "                                memory_efficient=memory_efficient)\n",
    "            self.features.add_module(\"denseblock%d\" % (i + 1), block)\n",
    "            num_features = num_features + num_layers * growth_rate\n",
    "\n",
    "            if i != len(block_config) - 1:\n",
    "                trans = _Transition(input_c=num_features,\n",
    "                                    output_c=num_features // 2)\n",
    "                self.features.add_module(\"transition%d\" % (i + 1), trans)\n",
    "                num_features = num_features // 2\n",
    "\n",
    "        # finnal batch norm\n",
    "        self.features.add_module(\"norm5\", nn.BatchNorm2d(num_features))\n",
    "\n",
    "        # fc layer\n",
    "        self.classifier = nn.Linear(num_features, num_classes)\n",
    "\n",
    "        # init weights\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "                \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        features = self.features(x)\n",
    "        out = F.relu(features, inplace=True)\n",
    "        out = F.adaptive_avg_pool2d(out, (1, 1))\n",
    "        out = torch.flatten(out, 1)\n",
    "        out = self.classifier(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "\n",
    "def densenet169(**kwargs: Any) -> DenseNet:\n",
    "    # Top-1 error: 24.00%\n",
    "    # 'densenet169': 'https://download.pytorch.org/models/densenet169-b2777c0a.pth'\n",
    "    return DenseNet(growth_rate=32,\n",
    "                    block_config=(6, 12, 32, 32),\n",
    "                    num_init_features=64,\n",
    "                    **kwargs)\n",
    "        \n",
    "device = \"cuda\"\n",
    "model = densenet169(num_classes = 1).to(device)\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0639d502-8dfe-4fa2-8e6c-0bbe3ed4584a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 6.995892433019785\n",
      "Epoch 2/10, Loss: 6.503943250729487\n",
      "Epoch 3/10, Loss: 6.067386601979916\n",
      "Epoch 4/10, Loss: 5.628240303351329\n",
      "Epoch 5/10, Loss: 5.154004864967786\n",
      "Epoch 6/10, Loss: 4.658680500892492\n",
      "Epoch 7/10, Loss: 4.190043096358959\n",
      "Epoch 8/10, Loss: 3.7071504042698784\n",
      "Epoch 9/10, Loss: 3.2481437520338936\n",
      "Epoch 10/10, Loss: 2.878898439499048\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch_indx, (inputs, labels) in enumerate(loaded_train_dataset):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # scheduler.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    # Print average loss for the epoch\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss / (len(loaded_train_dataset) / batch_size)}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94026d2f-72d7-4079-b24c-3185b623c316",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_probabilities = []\n",
    "true_labels = []\n",
    "with torch.set_grad_enabled(False):\n",
    "    for batch_indx, (inputs, labels) in enumerate(loaded_val_dataset):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)      \n",
    "        outputs = model(inputs)\n",
    "        predicted_probabilities.extend(outputs.tolist())\n",
    "        true_labels.extend(labels.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95bc1227-3076-4057-ba78-c02b67ada459",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics_output(preds,labels):\n",
    "    true_labels = np.array(labels)\n",
    "    predicted_probs = np.array(preds)\n",
    "    binary_predictions = (predicted_probs >= 0.5).astype(int)\n",
    "    auc = roc_auc_score(true_labels, predicted_probs)\n",
    "    conf_matrix = confusion_matrix(true_labels, binary_predictions)\n",
    "    tn, fp, fn, tp = conf_matrix.ravel()\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    specificity = tn / (tn + fp)\n",
    "    accuracy = accuracy_score(true_labels, binary_predictions)\n",
    "    f1 = f1_score(true_labels, binary_predictions)\n",
    "    mcc = matthews_corrcoef(true_labels, binary_predictions)  \n",
    "    return (auc, sensitivity, specificity, accuracy, f1, mcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ec41ab6-c044-4807-a1b3-c723a8123d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8600985221674878 0.7142857142857143 0.8620689655172413 0.78125 0.7812500000000001 0.5763546798029556\n"
     ]
    }
   ],
   "source": [
    "roc_auc, metrics_sn, metrics_sp, metrics_ACC, metrics_F1, metrics_MCC = metrics_output(predicted_probabilities, true_labels)\n",
    "print(roc_auc, metrics_sn, metrics_sp, metrics_ACC, metrics_F1, metrics_MCC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6d802a4-78db-4c1d-a174-efdfb218862b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('/root/autodl-tmp/ROC/RE-US/DenseNet-169/y_val_pred.npy', predicted_probabilities)\n",
    "np.save('/root/autodl-tmp/ROC/RE-US/DenseNet-169/y_val.npy', true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3eae8419-0110-45ee-bc57-fcb79224c187",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_probabilities = []  \n",
    "true_labels = []  \n",
    "with torch.set_grad_enabled(False): \n",
    "    for batch_indx, (inputs, labels) in enumerate(loaded_test_dataset):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)    \n",
    "        outputs = model(inputs)\n",
    "        predicted_probabilities.extend(outputs.tolist())\n",
    "        true_labels.extend(labels.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c9cb8da-d204-444f-a832-b8e446ef28c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.835625 0.725 0.875 0.8 0.7837837837837837 0.6068660849090084\n"
     ]
    }
   ],
   "source": [
    "roc_auc, metrics_sn, metrics_sp, metrics_ACC, metrics_F1, metrics_MCC = metrics_output(predicted_probabilities, true_labels)\n",
    "print(roc_auc, metrics_sn, metrics_sp, metrics_ACC, metrics_F1, metrics_MCC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a3ef595-e9b2-4fac-831a-89ecb704bb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('/root/autodl-tmp/ROC/RE-US/DenseNet-169/y_test_pred.npy', predicted_probabilities)\n",
    "np.save('/root/autodl-tmp/ROC/RE-US/DenseNet-169/y_test.npy', true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bd9b51-6110-4f7f-a730-0a966ca4fdc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e36799-ec3e-47b2-a1ee-34e260f7b5d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
